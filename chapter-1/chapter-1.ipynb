{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4d4f62d",
   "metadata": {},
   "source": [
    "#### Prerequisites: \n",
    "1. To follow along with the instructions in this chapter, you'll require a terminal.\n",
    "2. Change your working directory by typing the following command: ```cd /p/project/training2308/$USER/```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3ca1130-19c1-4bc4-bfbe-a1c1d087c09d",
   "metadata": {},
   "source": [
    "# Installation of Conda via Miniconda\n",
    "To facilitate your training using Python and TensorFlow, it's essential to set up an environment that can be uniformly accessed across various nodes. In this guide, we'll employ Miniconda for the creation of a Python virtual environment dedicated to your experiments.\n",
    "\n",
    "## Conducting Training using High-Performance Computing and TensorFlow\n",
    "Follow the linked steps below to effectively set up and conduct your training:\n",
    "\n",
    "1. [Download Miniconda](#Download-Miniconda)\n",
    "2. [Install Conda with the Downloaded Miniconda Script](#Install-Conda)\n",
    "3. [Verify the Usability of Python within the Conda Environment](#Python-Usability)\n",
    "4. [Formulate a Conda Environment utilizing the Supplied Yaml File](#Create-Conda-Environment)\n",
    "5. [Install the Necessary Packages](#Install-Packages)\n",
    "6. [Familiarize yourself with the Folder Structure and Files](#Folder-Structure-and-Files)\n",
    "7. [Amend the Configuration for Training](#Update-Configuration)\n",
    "8. [Modify the Batch_Job File](#Update-Batch_Job-File)\n",
    "9. [Initiate the Job Submission](#Submit-Job)\n",
    "10. [Keep Track of the Progress](#Monitor-Progress)\n",
    "11. [Validate the Saved Model](#Test-Saved-Model)\n",
    "\n",
    "## Utilizing Cloud Computing for Inference\n",
    "To leverage the power of cloud computing for inference, follow the outlined steps:\n",
    "\n",
    "1. [Transfer the Trained Model to an AWS Environment via boto3](#Push-Model-to-AWS)\n",
    "2. [Access Configured Environments in SageMaker](#Access-SageMaker-Environments)\n",
    "3. [Load your Model into SageMaker](#Load-Model-in-SageMaker)\n",
    "4. [Execute Model Deployment](#Deploy-Model)\n",
    "5. [Examine the Deployed Model](#Test-Deployed-Model)\n",
    "6. [Establish an API Endpoint for Model Interaction](#Deploy-API-Endpoint)\n",
    "7. [Engage with the API Endpoint to Extract Inferences from the Model](#Interact-with-API)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f788d9d8",
   "metadata": {},
   "source": [
    "# Download Miniconda\n",
    "In this step, we will download Miniconda, which we will use to create our Python virtual environment. You can download it from the following link: [Miniconda](https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe5470b",
   "metadata": {},
   "source": [
    "# Install Conda\n",
    "After downloading Miniconda, we will install Conda. Use the script that you downloaded in the previous step to install Conda. We will walk through the steps of this process in this section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcc3966",
   "metadata": {},
   "source": [
    "# Python Usability\n",
    "In this section, we will verify that we can use Python from the Conda environment that we have set up. This involves testing our setup and ensuring that Python is properly installed and can be accessed from our environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b823b42d",
   "metadata": {},
   "source": [
    "# Create Conda Environment\n",
    "Next, we will create a new Conda environment for our project. This environment will isolate our project's dependencies from other Python projects on our machine. We will use a provided YAML file to define our environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be845c8",
   "metadata": {},
   "source": [
    "# Install Packages\n",
    "Once our environment is set up, we need to install the necessary Python packages for our project. These packages will allow us to run our TensorFlow training and include packages like TensorFlow, NumPy, and others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bd3a6e",
   "metadata": {},
   "source": [
    "# Folder Structure and Files\n",
    "Now, let's go over the structure of our project. We'll discuss the organization of our project files and folders, which will include our training scripts, configuration files, and more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f2e236",
   "metadata": {},
   "source": [
    "# Update Configuration\n",
    "Before we can start training, we need to update our configuration. This could include setting the paths to our training and validation datasets, setting our learning rate, and other hyperparameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b38266",
   "metadata": {},
   "source": [
    "# Update Batch_Job File\n",
    "In this step, we will update our batch job file. This is the file that we will submit to our high-performance computing (HPC) cluster to start our training job.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc78c23",
   "metadata": {},
   "source": [
    "# Submit Job\n",
    "Next, we will submit our training job to the HPC cluster. We will use the batch job file that we updated in the previous step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c26a7a",
   "metadata": {},
   "source": [
    "# Monitor Progress\n",
    "Once our job is submitted, we need to monitor its progress. We can do this by checking the output logs from our job, as well as any job status information provided by the HPC cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bc28cd",
   "metadata": {},
   "source": [
    "# Test Saved Model\n",
    "After our training job is completed, we will test the saved model to ensure that it is working correctly. This will involve loading the model from the saved file and running it on some test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1429ffcb",
   "metadata": {},
   "source": [
    "# Push Model to AWS\n",
    "In this step, we will transfer our trained model to an AWS environment using boto3. This will allow us to use cloud resources to run our inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf291082",
   "metadata": {},
   "source": [
    "# Access SageMaker Environments\n",
    "Next, we will access our pre-configured environments in Amazon SageMaker. We will use these environments to load and run our model in the cloud.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1538633",
   "metadata": {},
   "source": [
    "# Load Model in SageMaker\n",
    "In this section, we will load our model into SageMaker. This involves transferring our model file to a location that SageMaker can access, and then telling SageMaker to load the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6945041e",
   "metadata": {},
   "source": [
    "# Deploy Model\n",
    "Once our model is loaded in SageMaker, we can deploy it. This will make our model available for inference requests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1905042e",
   "metadata": {},
   "source": [
    "# Test Deployed Model\n",
    "After deploying our model, we will test it to ensure it is working correctly. This involves sending some test requests to our deployed model and checking the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae9e24d",
   "metadata": {},
   "source": [
    "# Deploy API Endpoint\n",
    "Next, we will deploy an API endpoint that we can use to interact with our model. This API will allow us to send requests to our model and receive the model's predictions in response.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cf9032",
   "metadata": {},
   "source": [
    "# Interact with API\n",
    "In the final section, we will interact with our deployed API endpoint. We will send some test requests to the API and display the inferences made by our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28b2437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939c2164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b756d296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce86dc67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e771ee6a-3231-4174-a1de-5881d7a5d9a2",
   "metadata": {},
   "source": [
    "## Download Miniconda from https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e32d3967",
   "metadata": {},
   "source": [
    "All of the following sections are to be performed in a terminal (from the jupyterhub). <Add details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374e44b4-2101-42e0-8260-7cb075ea306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70164300-f584-4d32-9938-3ea6efb4728e",
   "metadata": {},
   "source": [
    "## Install conda using the downloaded Miniconda shell script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b18c800-4ad6-42a6-b338-b1c72a1a8eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "chmod 770 Miniconda3-latest-Linux-x86_64.sh\n",
    "./Miniconda3-latest-Linux-x86_64.sh\n",
    "# Where to install miniconda (after the installation, it will ask): `/p/project/training2206/$USER/miniconda3` \n",
    "# Do you wish the installer to Initialize miniconda3? Yes \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5783e3d5-1a5e-4a7a-98b6-c3f1574bc67d",
   "metadata": {},
   "source": [
    "### Check if the installation updated your bashrc to automatically use Python from conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2c4600-e356-4613-9f2e-69a2145339f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat ~/.bashrc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc3b96b9-db5b-40fd-8422-43bbac01dda2",
   "metadata": {},
   "source": [
    "## Folder structure\n",
    "\n",
    "All of the project-related files are located at `/p/project/training2308/$USER/`. (User is the environment variable with your user name. You can check what it is set as using `echo $USER`)\n",
    "\n",
    "Change directory to the aforementioned directory.\n",
    "\n",
    "Check for `2023-igarss-tutorial` folder in the directory. If it is not present, you can use Git to download it from https://github.com/nasa-impact/2023-igarss-tutorial using `git clone https://github.com/nasa-impact/pixel-detector.git` or using the jupyterhub.\n",
    "\n",
    "Once cloned, change the directory to the 2023-igarss-tutorial folder using `cd 2023-igarss-tutorial`\n",
    "\n",
    "Below is the folder structure for the code:\n",
    "```\n",
    "|> chapter-1\n",
    "    |> mmsegmentation\n",
    "        |> config: `Contains configuration files`\n",
    "        |> burn_scars.sh `Bash file for Burn Scars Training job submission.`\n",
    "        |> flood.sh `Bash file for Flood Training job submission.`\n",
    "    ...\n",
    "|> chapter-2 `Contains files for loading the files in sagemaker environment and inferencing.`\n",
    "|> chapter-3 `Contains files for establishing an API in cloud environment to interact with the trained model`\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b14cf13a-137d-4f5c-b2cd-610b95371b63",
   "metadata": {},
   "source": [
    "## Create conda environment\n",
    "You will use the Python from the conda environment to create a conda environment which will be used throughout.\n",
    "\n",
    "In some cases, conda might not be activated after installation. You can just refresh your bash terminal using `exec bash`, and it should enable the conda environment for you.\n",
    "\n",
    "Once in the conda environment, you can create a new Python virtual environment using `conda create --name py39 -f /p/project/training2308/tutorial.yml`\n",
    "\n",
    "Then you will use the environment you just created using `conda activate py39`\n",
    "\n",
    "Once the environment is activated, you will need to make sure you are starting from scratch. To make sure no other modules are installed, use `module purge` to remove all the unwanted modules.\n",
    "\n",
    "For the purposes of this tutorial all packages except mmcv, mmcv-full, and mmsegmentation are already prepared via the tutorial.yml file.\n",
    "\n",
    "We will need to install the aforementioned files as follows:\n",
    "```\n",
    "mim install mmcv==1.5.0\n",
    "mim install mmcv-full==1.5.0\n",
    "mim install mmsegmentation==0.30.0\n",
    "```\n",
    "Once these are installed, local version of mmsegmentation also needs to be installed:\n",
    "`cd mmsegmentation`\n",
    "`pip install -e .`\n",
    "`cd ..`\n",
    "\n",
    "We also need boto3 for the models to be transfered to cloud environment.\n",
    "`pip install boto3`\n",
    "\n",
    "All of the required packages are now installed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b857057d-6d7c-4a6d-b283-875a8eee8efe",
   "metadata": {},
   "source": [
    "## Update configuration and environment variables\n",
    "\n",
    "To find your user name, run the following command in the terminal:\n",
    "\n",
    "`echo $USER`\n",
    "\n",
    "Training and validation files are located at `/p/project/training2308/data/burn_scars` and `/p/project/training2308/data/flood`.\n",
    "\n",
    "Before we start working on any of this, we will change our directory to `/p/project/training2206/$USER/2023-igarss-tutorial/chapter-1`\n",
    "\n",
    "In the jupyter lab interface, find `chapter-1/burn_scars.sh` or `chapter-1/flood.sh` file. Right click on either of them in the left pane, and select `editor`. Once the file is open, you can update the `<username>` instances with your `username`.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73d17936-ef79-4293-9d2d-74b2a60557ea",
   "metadata": {},
   "source": [
    "# Submit Training Job\n",
    "In the `burn_scars.sh` or `flood.sh` file you can specify the number of nodes you want to use for training. As an example, you are going to use 2 nodes for training.\n",
    "\n",
    "Check details of the training job:\n",
    "\n",
    "`cat /p/project/training2206/$USER/2023-igarss-tutorial/chapter-1/burn_scars.sh`\n",
    "\n",
    "You can submit the training job using the `sbatch` command. Like so: `sbatch burn_scars.sh` or `sbatch flood.sh`\n",
    "\n",
    "Once submitted, two new files will be created by the process: `output.out` and `error.err`. `output.out` will contain details of the output from your processes, and `error.err` will provide details on any errors or logs from the scripts. Once the job is submitted and the files are created, you can check for updates simply by using `tail -f output.out error.err`. (Any warnings, automated messages, and errors are tracked in the `error.err` file while only the [ed. note: incomplete sentence]\n",
    "\n",
    "You can see how good or bad the model training is by watching the loss outputs in `output.out` or `error.err`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc9a1f02",
   "metadata": {},
   "source": [
    "# Uploading the Model to a Cloud Environment\n",
    "\n",
    "After the model is finished training, the model is stored in the location specified in your config file `/p/project/training2308/<username>/<experiment>/training/latest.pth`, where `<username>` is your `username` and `experiment` is one of `burn_scars` or `flood`. You will be compressing this model and pushing it to an S3 bucket using `boto3` and the credentials from the AWS account shared with you.\n",
    "\n",
    "## Compress file\n",
    "- We will first change the name of the file to a consistent convention.         \n",
    "```cp /p/project/training2308/<username>/<experiment>/Experiment_<timestamp>/training/latest.pth /p/project/training2308/<username>/<username>_<experiment>.pth```\n",
    "\n",
    "- We will use `tar` to compress the file: \n",
    "```tar -czvf /p/project/training2308/<username>/<username>_<experiment>.tar.gz /p/project/training2308/<username>/<username>_<experiment>.pth```\n",
    "\n",
    "\n",
    "## Get AWS credentials\n",
    "Account creation links should have been shared with you. Once the account is setup, you can obtain the credentials required for upload from the AWS SSO homepage.\n",
    "Please follow the steps listed below:\n",
    "\n",
    "1. Navigate to https://nasa-impact.awsapps.com/start\n",
    "2. Login\n",
    "3. Click on `AWS Account`\n",
    "4. Click on `Summer School`\n",
    "5. Click on `Command line or Programmatic access`\n",
    "6. Copy the `AWS Access Key Id`, `AWS Secret Access Key`, and `AWS session token` from the pop up\n",
    "7. Update the following script and run it in a python shell. (You can start a python shell by just typing `python` in the terminal).\n",
    "\n",
    "This will upload the files directly into the S3 bucket. You will then fetch the file from S3 bucket into the SageMaker notebook from where you will be deploying the model and hosting an API to interact with the model.\n",
    "\n",
    "\n",
    "*Note: Please make sure the virtual environment is active while working with the python shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9503e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "AWS_ACCESS_KEY_ID = \"<Copied over from SSO login>\"\n",
    "AWS_SECRET_ACCESS_KEY = \"<Copied over from SSO login>\"\n",
    "AWS_SESSION_TOKEN = \"<Copied over from SSO login>\"\n",
    "\n",
    "BUCKET_NAME = \"2023-igarss-tutorial-store\"\n",
    "## Please update this with either burn_scars or flood\n",
    "EXPERIMENT = \"<experiment_name>\"\n",
    "\n",
    "USER = os.environ.get(\"USER\")\n",
    "\n",
    "\n",
    "def generate_federated_session():\n",
    "    \"\"\"\n",
    "    Method to generate federated session to upload the file from HPC to S3 bucket.\n",
    "    ARGs:\n",
    "        filename: Upload filename\n",
    "    Returns:\n",
    "        Signed URL for file upload\n",
    "    \"\"\"\n",
    "    return boto3.session.Session(\n",
    "        aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "        aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "        aws_session_token=AWS_SESSION_TOKEN,\n",
    "    )\n",
    "\n",
    "\n",
    "model_filename = f\"{USER}/{USER}_{EXPERIMENT}.tar.gz\"\n",
    "model_path = f\"/p/project/training2308/{model_filename}\"\n",
    "session = generate_federated_session()\n",
    "s3_connector = session.client(\"s3\")\n",
    "\n",
    "s3_connector.upload_file(model_path, BUCKET_NAME, f\"{USER}_{EXPERIMENT}.tar.gz\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c67866e",
   "metadata": {},
   "source": [
    "Once the process is done, you can check for the files in S3 using the AWS console.\n",
    "\n",
    "1. Navigate to https://nasa-impact.awsapps.com/start\n",
    "2. Login\n",
    "3. Click on `AWS Account`\n",
    "4. Click on `Summer School`\n",
    "5. Click on `Management Console`\n",
    "6. In the search bar, search for `s3`\n",
    "7. Click on `s3`\n",
    "8. Click on `2023-igarss-tutorial-data`\n",
    "9. Click on your `username`\n",
    "\n",
    "You should be able to view your file there now. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ec7e0a2",
   "metadata": {},
   "source": [
    "# Foundation Models\n",
    "\n",
    "Foundation models represent a significant change in the realm of deep learning. Unlike traditional methods that train models using labeled datasets, foundation models leverage extensive volumes of broad, unlabeled data. Pretraining these models on sizable datasets facilitates faster adaptation to downstream tasks with less labeled data. This is particularly beneficial when dealing with remote sensing data, where labeled datasets are rare and costly to acquire.\n",
    "\n",
    "# ![image.png](images/dlvsfm.png)\n",
    "Figure 1. A depiction of a foundation model pre-trained on a large unlabeled dataset. The model is subsequently fine-tuned for a specific downstream task using a smaller labeled dataset.\n",
    "\n",
    "## Transformers\n",
    "The renowned transformer model architecture is currently the cornerstone for foundation models. It's a type of neural network architecture that employs attention mechanisms (as seen in Fig. 2) to comprehend contextual relationships between different parts of the input. Transformers have demonstrated great effectiveness in diverse tasks such as language translation, text summarization, and image captioning, within the NLP domain.\n",
    "\n",
    "![image.png](images/attention.webp)\n",
    "Figure 2. Fundamental transformer architecture. The attention mechanism enables the model to understand contextual relationships between inputs.\n",
    "\n",
    "## Vision Transformers\n",
    "In the realm of computer vision, Convolutional Neural Networks (CNNs) have been the preferred architecture for many years, primarily due to their efficiency and ability to emulate the visual cortex. However, with the success of transformers in the Natural Language Processing (NLP) domain (including BERT, BART, GPT, LLaMa), Vision Transformers (ViTs) have been introduced as a novel architecture for computer vision tasks, with performance on-par or exceeding CNNs. Here, images are viewed as sequences of image patches. Feature maps are modeled as vectors of tokens, each token representing an embedding of a specific image patch. ViTs have an edge over CNNs because they are less subject to the inductive biases that CNNs face (like assumptions of locality and weight sharing). Moreover, ViTs possess a larger global receptive field (the ability to 'attend' to the entire image simultaneously). One disadvantage of ViTs is their computational expense for training, requiring substantial compute and memory resources. CNNs, on the other hand, benefit from years of operator and library optimization, making them more efficient to train.\n",
    "\n",
    "![image.png](images/vit.png)\n",
    "Figure 3. ViT architecture. The image is divided into patches and flattened into a sequence of tokens. These tokens are then fed into a transformer encoder.\n",
    "\n",
    "## Pretraining Vision Transformers\n",
    "\n",
    "The pretraining of transformers helps the model learn how to extract features from unlabeled data. This process generally employs two main strategies: self-supervised contrastive learning (CL) and Masked Image Modeling (MIM). CL typically involves augmenting an image and minimizing the distance between the augmented image and the original image in the embedding space. Conversely, MIM involves masking a portion of the image and predicting the masked section. The pretraining process is illustrated in Fig. 4.\n",
    "\n",
    "![image.png](images/mim.png)\n",
    "Figure 4. The Masked Image Modeling pretraining process. The image is divided into patches and flattened into a sequence of tokens. A segment of the tokens is masked, and the model is trained to predict the full image, including the masked tokens.\n",
    "\n",
    "\n",
    "\n",
    "## A geospatial foundation model for Harmonized Landsat Sentinel-2 (HLS) data\n",
    "\n",
    "The NASA-IMPACT, in partnership with IBM developed a Foundation model based on Harmonized Landsat Sentinel-2 (HLS) data. HLS is a global, harmonized surface reflectance product derived from Landsat 8 and Sentinel-2 data. The HLS Foundation Model is a ViT based model pre-trained on HLS data. The model is trained using the MIM pretraining strategy. The model is trained on 30m resolution HLS data and can be fine-tuned for downstream tasks such as land cover classification, change detection, object detection and segmentation. The HLS FM architecture is illustrated in Fig. 5.\n",
    "\n",
    "![image.png](images/hls_fm.png)\n",
    "Figure 5. The HLS Foundation Model architecture.\n",
    "\n",
    "### Model Details:\n",
    "```\n",
    "Temporal Range -US for the year of 2017 \n",
    "Spectral bands - 6 (Blue, Green, Red, NIRnarrow, SWIR1, SWIR2)\n",
    "Model size  - 100 million parameters\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ef01ad3",
   "metadata": {},
   "source": [
    "# Notes on Fine-tuning\n",
    "\n",
    "Fine-tuning Vision Foundation Models:\n",
    "\n",
    "Once a vision foundation model, like a Vision Transformer (ViT), has been pretrained on a large, unlabeled dataset, it's ready to be fine-tuned for specific tasks. Fine-tuning is a procedure that tailors a pretrained model to work on a particular task with a smaller labeled dataset. This process allows us to leverage the knowledge gained by the model during pretraining and adjust it to many tasks. Here's how it generally works:\n",
    "\n",
    "1. **Task-Specific Dataset Preparation**: First step requires compilation of labeled dataset for a specific task on interest. This dataset is typically smaller than what is required for training a model from scratch, but needs to be similar in signature to the original pretraining dataset. It could be a collection of images labeled with classes for object recognition, bounding boxes for object detection, or any other labeling that corresponds to the specific vision task.\n",
    "\n",
    "2. **Initialize with Pretrained Weights**: We begin by initializing your vision transformer model with the weights learned during pretraining. This is the primary difference between training a model from scratch and fine-tuning - instead of starting with random weight initialization, we are starting with weights that already have some representation of the data.\n",
    "\n",
    "3. **Model Adjustment by Fine Tuning**: During the fine-tuning process, the weights of the pretrained model are adjusted using backpropagation and gradient descent, as with normal training. However, since the model is already close to the optimal solution, the updates made to the weights during training are typically quite small. The simplest approach is to fine-tune all the layers of the model. However, we can also choose to freeze (or hold constant) the earlier layers of the model (which often capture more generic features), and only fine-tune some of the later layers (which capture more task-specific features). You can also add one or more fully-connected layers at the end of the model and only train these layers on the new data. Typically very small learning rate is employed to preserve the original weights (this is a problem known as catastrophic forgetting) and avoid overfitting.\n",
    "\n",
    "4. **Evaluation and Iteration**: Finally, the fine-tuned model is evaluated on a separate test set to measure its performance. \n",
    "By leveraging pre-trained models, we are able to reduce the amount of labeled data required to train a model for a specific task. This is particularly useful in the remote sensing domain, where labeled datasets are often scarce and costly to acquire. We also spend less time and effort in training the model due to pre-emptive compute. \n",
    "\n",
    "## MMSegmentation\n",
    "\n",
    "We use the popular segmentation library, [MMSegmentation](https://github.com/open-mmlab/mmsegmentation) for finetuning our Vision transformer. MMSegmentation is a part of the [OpenMMLab](https://github.com/open-mmlab) computer vision library. While the library supports various backbones and methods of segmentation, we particularly used ViT backbone with MAE masking strategy. For segmentation method, [U-Net](https://arxiv.org/abs/1505.04597) architecture was used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8987afb44532b2110e1a5e1b229dd281f8440b44477d285826a54acdd52d8797"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
