{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4d4f62d",
   "metadata": {},
   "source": [
    "#### Note: \n",
    "1. You will be working off of terminal for this chapter.\n",
    "2. Change your working directory: ```cd /p/project/training2308/$USER/```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ca1130-19c1-4bc4-bfbe-a1c1d087c09d",
   "metadata": {},
   "source": [
    "# Install Monda using Miniconda\n",
    "You will be using Python and TensorFlow for training. For this you need to create an environment you can use across nodes. You will be using Miniconda to create a Python virtual environment for your experiments.\n",
    "\n",
    "## Training using high performance computing and TensorFlow\n",
    "1. Download Miniconda from https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "2. Install conda using the Miniconda script you downloaded\n",
    "3. Make sure you can use the Python from the conda environment\n",
    "4. Create a conda environment using provided yaml file\n",
    "5. Install required packages\n",
    "6. Folder structure and files you will be using\n",
    "7. Update configuration for training\n",
    "8. Update batch_job file.\n",
    "9. Submit job\n",
    "10. Check progress\n",
    "11. Test saved model\n",
    "\n",
    "## Use cloud computing for inferencing\n",
    "1. Push trained model to AWS environment using boto3 (share credentials before this step)\n",
    "2. Access setup environments in SageMaker\n",
    "3. Load model in SageMaker\n",
    "4. Deploy model\n",
    "5. Test deployed model\n",
    "6. Deploy API endpoint to interact with model\n",
    "7. Interact with API endpoint to get inferences from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e771ee6a-3231-4174-a1de-5881d7a5d9a2",
   "metadata": {},
   "source": [
    "## Download Miniconda from https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32d3967",
   "metadata": {},
   "source": [
    "All of the following sections are to be performed in a terminal (from the jupyterhub). <Add details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374e44b4-2101-42e0-8260-7cb075ea306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70164300-f584-4d32-9938-3ea6efb4728e",
   "metadata": {},
   "source": [
    "## Install conda using the downloaded Miniconda shell script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b18c800-4ad6-42a6-b338-b1c72a1a8eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "chmod 770 Miniconda3-latest-Linux-x86_64.sh\n",
    "./Miniconda3-latest-Linux-x86_64.sh\n",
    "# Where to install miniconda (after the installation, it will ask): `/p/project/training2206/$USER/miniconda3` \n",
    "# Do you wish the installer to Initialize miniconda3? Yes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5783e3d5-1a5e-4a7a-98b6-c3f1574bc67d",
   "metadata": {},
   "source": [
    "### Check if the installation updated your bashrc to automatically use Python from conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2c4600-e356-4613-9f2e-69a2145339f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3b96b9-db5b-40fd-8422-43bbac01dda2",
   "metadata": {},
   "source": [
    "## Folder structure\n",
    "\n",
    "All of the project-related files are located at `/p/project/training2308/$USER/`. (User is the environment variable with your user name. You can check what it is set as using `echo $USER`)\n",
    "\n",
    "Change directory to the aforementioned directory.\n",
    "\n",
    "Check for `2023-igarss-tutorial` folder in the directory. If it is not present, you can use Git to download it from https://github.com/nasa-impact/2023-igarss-tutorial using `git clone https://github.com/nasa-impact/pixel-detector.git` or using the jupyterhub.\n",
    "\n",
    "Once cloned, change the directory to the 2023-igarss-tutorial folder using `cd 2023-igarss-tutorial`\n",
    "\n",
    "Below is the folder structure for the code:\n",
    "```\n",
    "|> chapter-1\n",
    "    |> mmsegmentation\n",
    "        |> config: `Contains configuration files`\n",
    "        |> burn_scars.sh `Bash file for Burn Scars Training job submission.`\n",
    "        |> flood.sh `Bash file for Flood Training job submission.`\n",
    "    ...\n",
    "|> chapter-2 `Contains files for loading the files in sagemaker environment and inferencing.`\n",
    "|> chapter-3 `Contains files for establishing an API in cloud environment to interact with the trained model`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14cf13a-137d-4f5c-b2cd-610b95371b63",
   "metadata": {},
   "source": [
    "## Create conda environment\n",
    "You will use the Python from the conda environment to create a conda environment which will be used throughout.\n",
    "\n",
    "In some cases, conda might not be activated after installation. You can just refresh your bash terminal using `exec bash`, and it should enable the conda environment for you.\n",
    "\n",
    "Once in the conda environment, you can create a new Python virtual environment using `conda create --name py39 -f tutorial.yml`\n",
    "\n",
    "Then you will use the environment you just created using `conda activate py39`\n",
    "\n",
    "Once the environment is activated, you will need to make sure you are starting from scratch. To make sure no other modules are installed, use `module purge` to remove all the unwanted modules.\n",
    "\n",
    "For the purposes of this tutorial all packages except mmcv, mmcv-full, and mmsegmentation are already prepared via the tutorial.yml file.\n",
    "\n",
    "We will need to install the aforementioned files as follows:\n",
    "```\n",
    "mim install mmcv==1.5.0\n",
    "mim install mmcv-full==1.5.0\n",
    "mim install mmsegmentation==0.30.0\n",
    "```\n",
    "Once these are installed, local version of mmsegmentation also needs to be installed:\n",
    "`cd mmsegmentation`\n",
    "`pip install -e .`\n",
    "`cd ..`\n",
    "\n",
    "All of the required packages are now installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b857057d-6d7c-4a6d-b283-875a8eee8efe",
   "metadata": {},
   "source": [
    "## Update configuration and environment variables\n",
    "\n",
    "To find your user name, run the following command in the terminal:\n",
    "\n",
    "`echo $USER`\n",
    "\n",
    "Training and validation files are located at `/p/project/training2308/data/burn_scars` and `/p/project/training2308/data/flood`.\n",
    "\n",
    "Before we start working on any of this, we will change our directory to `/p/project/training2206/$USER/2023-igarss-tutorial/chapter-1`\n",
    "\n",
    "In the jupyter lab interface, find `chapter-1/burn_scars.sh` or `chapter-1/flood.sh` file. Right click on either of them in the left pane, and select `editor`. Once the file is open, you can update the `<username>` instances with your `username`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d17936-ef79-4293-9d2d-74b2a60557ea",
   "metadata": {},
   "source": [
    "# Submit Training Job\n",
    "In the `burn_scars.sh` or `flood.sh` file you can specify the number of nodes you want to use for training. As an example, you are going to use 2 nodes for training.\n",
    "\n",
    "Check details of the training job:\n",
    "\n",
    "`cat /p/project/training2206/$USER/2023-igarss-tutorial/chapter-1/burn_scars.sh`\n",
    "\n",
    "You can submit the training job using the `sbatch` command. Like so: `sbatch burn_scars.sh` or `sbatch flood.sh`\n",
    "\n",
    "Once submitted, two new files will be created by the process: `output.out` and `error.err`. `output.out` will contain details of the output from your processes, and `error.err` will provide details on any errors or logs from the scripts. Once the job is submitted and the files are created, you can check for updates simply by using `tail -f output.out error.err`. (Any warnings, automated messages, and errors are tracked in the `error.err` file while only the [ed. note: incomplete sentence]\n",
    "\n",
    "You can see how good or bad the model training is by watching the loss outputs in `output.out` or `error.err`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9a1f02",
   "metadata": {},
   "source": [
    "# Uploading the Model to a Cloud Environment\n",
    "\n",
    "After the model is finished training, the model is stored in the location specified in your config file `/p/project/training2308/<username>/<experiment>/training/latest.pth`, where `<username>` is your `username` and `experiment` is one of `burn_scars` or `flood`. You will be taking this model and pushing it to an S3 bucket using `boto3` and the credentials from the AWS account shared with you.\n",
    "\n",
    "## Get AWS credentials\n",
    "Account creation links should have been shared with you. Once the account is setup, you can obtain the credentials required for upload from the AWS SSO homepage.\n",
    "Please follow the steps listed below:\n",
    "\n",
    "1. Navigate to https://nasa-impact.awsapps.com/start\n",
    "2. Login\n",
    "3. Click on `AWS Account`\n",
    "4. Click on `Summer School`\n",
    "5. Click on `Command line or Programmatic access`\n",
    "6. Copy the `AWS Access Key Id`, `AWS Secret Access Key`, and `AWS session token` from the pop up\n",
    "7. Update the following script and run it in a python shell. (You can start a python shell by just typing `python` in the terminal).\n",
    "\n",
    "This will upload the files directly into the S3 bucket. You will then fetch the file from S3 bucket into the SageMaker notebook from where you will be deploying the model and hosting an API to interact with the model.\n",
    "\n",
    "\n",
    "*Note: Please make sure the virtual environment is active while working with the python shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9503e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 \n",
    "import os\n",
    "\n",
    "AWS_ACCESS_KEY_ID = <Copied over from SSO login>\n",
    "AWS_SECRET_ACCESS_KEY = <Copied over from SSO login>\n",
    "AWS_SESSION_TOKEN = <Copied over from SSO login>\n",
    "\n",
    "BUCKET_NAME = '2023-igarss-tutorial-store'\n",
    "## Please update this with either burn_scars or flood\n",
    "EXPERIMENT = <experiment>\n",
    "\n",
    "USER = os.environ.get('USER')\n",
    "\n",
    "def generate_federated_session():\n",
    "    \"\"\"\n",
    "    Method to generate federated session to upload the file from HPC to S3 bucket.\n",
    "    ARGs:\n",
    "        filename: Upload filename\n",
    "    Returns: \n",
    "        Signed URL for file upload \n",
    "    \"\"\"\n",
    "    return boto3.session.Session(\n",
    "            aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "            aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "            aws_session_token=AWS_SESSION_TOKEN\n",
    "        )\n",
    "\n",
    "model_filename = f\"/p/project/training2308/{USER}/{EXPERIMENT}/training/latest.pth\"\n",
    "session = generate_federated_session()\n",
    "s3_connector = session.client('s3')\n",
    "\n",
    "s3_connector.upload_file(model_filename, BUCKET_NAME, f\"{USER}/{USER}_{EXPERIMENT}.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c67866e",
   "metadata": {},
   "source": [
    "Once the process is done, you can check for the files in S3 using the AWS console.\n",
    "\n",
    "1. Navigate to https://nasa-impact.awsapps.com/start\n",
    "2. Login\n",
    "3. Click on `AWS Account`\n",
    "4. Click on `Summer School`\n",
    "5. Click on `Management Console`\n",
    "6. In the search bar, search for `s3`\n",
    "7. Click on `s3`\n",
    "8. Click on `2023-igarss-tutorial-data`\n",
    "9. Click on your `username`\n",
    "\n",
    "You should be able to view your file there now. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec7e0a2",
   "metadata": {},
   "source": [
    "# Foundation Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef01ad3",
   "metadata": {},
   "source": [
    "# Notes on Fine-tuning\n",
    "\n",
    "## MMSegmentation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8987afb44532b2110e1a5e1b229dd281f8440b44477d285826a54acdd52d8797"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
