{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from metrics import metrics\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_test_img = 90\n",
    "path_to_input = \"/dccstor/geofm-finetuning/flood_mapping/inferences/inputs/input{0}.npy\"\n",
    "path_to_pred = \"/dccstor/geofm-finetuning/flood_mapping/inferences/pred{0}.npy\"\n",
    "path_to_label = \"/dccstor/geofm-finetuning/flood_mapping/inferences/label{0}.npy\"\n",
    "\n",
    "inputs = list()\n",
    "preds = list()\n",
    "labels = list()\n",
    "\n",
    "for index in range(n_test_img):\n",
    "    inputs.append(np.load(path_to_input.format(index)))\n",
    "    preds.append(np.load(path_to_pred.format(index)))\n",
    "    labels.append(np.load(path_to_label.format(index)))\n",
    "\n",
    "assert len(preds)==n_test_img, \"Number of images does not match number of predictions\"\n",
    "assert len(labels)==n_test_img, \"Number of images does not match number of labels\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_examples = 5\n",
    "rand_idx = np.random.randint(low=0, high=n_test_img, size=n_examples)\n",
    "f, axarr = plt.subplots(n_examples,3,figsize=(18, 18))\n",
    "\n",
    "for i, img_index in enumerate(rand_idx):\n",
    "    input_img = inputs[img_index].squeeze()[0:3, :,].transpose([1, 2, 0])\n",
    "    axarr[i,0].imshow(input_img)\n",
    "    axarr[i,1].imshow(preds[img_index]*255, cmap=\"gray\")\n",
    "    axarr[i,2].imshow(labels[img_index]*120, cmap=\"gray\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_iou = list()\n",
    "test_accuracy = list()\n",
    "test_bal_accuracy = list()\n",
    "test_precision = list()\n",
    "test_precision_weighted = list()\n",
    "test_recall = list()\n",
    "test_recall_weighted = list()\n",
    "test_f1 = list()\n",
    "test_f1_micro = list()\n",
    "test_f1_macro = list()\n",
    "test_f0_5 = list()\n",
    "test_f0_1 = list()\n",
    "test_f10 = list()\n",
    "test_precision_class_1 = list()\n",
    "test_precision_class_2 = list()\n",
    "test_recall_class_1 = list()\n",
    "test_recall_class_2 = list()\n",
    "test_fscore_class_1 = list()\n",
    "test_fscore_class_2 = list()\n",
    "test_iou_class_1 = list()\n",
    "test_iou_class_2 = list()\n",
    "\n",
    "for index, pred in enumerate(preds):\n",
    "    accuracy, bal_accuracy, precision, precision_weighted, recall, recall_weighted, iou_score, f1_score, f1_micro, f1_macro, f0_5, f0_1, f10, precision_per_class, recall_per_class, fscore_per_class, support_per_class, iou_score_per_class = metrics(\n",
    "        y_pred=pred, y_true=labels[index])\n",
    "    test_iou.append(iou_score)\n",
    "    test_accuracy.append(accuracy)\n",
    "    test_bal_accuracy.append(bal_accuracy)\n",
    "    test_precision.append(precision)\n",
    "    test_precision_weighted.append(precision_weighted)\n",
    "    test_recall.append(recall)\n",
    "    test_recall_weighted.append(recall_weighted)\n",
    "    test_f1.append(f1_score)\n",
    "    test_f1_micro.append(f1_micro)\n",
    "    test_f1_macro.append(f1_macro)\n",
    "    test_f0_5.append(f0_5)\n",
    "    test_f0_1.append(f0_1)\n",
    "    test_f10.append(f10)\n",
    "\n",
    "    if len(precision_per_class) > 1:\n",
    "        test_precision_class_1.append(precision_per_class[0])\n",
    "        test_precision_class_2.append(precision_per_class[1])\n",
    "        test_recall_class_1.append(recall_per_class[0])\n",
    "        test_recall_class_2.append(recall_per_class[1])\n",
    "        test_fscore_class_1.append(fscore_per_class[0])\n",
    "        test_fscore_class_2.append(fscore_per_class[1])\n",
    "        test_iou_class_1.append(iou_score_per_class[0])\n",
    "        test_iou_class_2.append(iou_score_per_class[1])\n",
    "\n",
    "results_df = pd.DataFrame({\"Test IoU\": [np.mean(test_iou)],\n",
    "                           \"Test Acc\": [np.mean(test_accuracy)],\n",
    "                           \"Test Prec\": [np.mean(test_precision)],\n",
    "                           \"Test Prec Weighted\": [np.mean(test_precision_weighted)],\n",
    "                           \"Test Recall\": [np.mean(test_recall)],\n",
    "                           \"Test Recall Weighted\": [np.mean(test_recall_weighted)],\n",
    "                           \"Test Bal Acc\": [np.mean(test_bal_accuracy)],\n",
    "                           \"Test F1\": [np.mean(test_f1)],\n",
    "                           \"Test f1_micro\": [np.mean(test_f1_micro)],\n",
    "                           \"Test f1_macro\": [np.mean(test_f1_macro)],\n",
    "                           \"Test F0.1\": [np.mean(test_f0_1)],\n",
    "                           \"Test F0.5\": [np.mean(test_f0_5)],\n",
    "                           \"Test F10\": [np.mean(test_f10)],\n",
    "                           \"Test F1 class 1\": [np.mean(test_fscore_class_1)],\n",
    "                           \"Test F1 class 2\": [np.mean(test_fscore_class_2)],\n",
    "                           \"Test Recall class 1\": [np.mean(test_recall_class_1)],\n",
    "                           \"Test Recall class 2\": [np.mean(test_recall_class_2)],\n",
    "                           \"Test Precision class 1\": [np.mean(test_precision_class_1)],\n",
    "                           \"Test Precision class 2\": [np.mean(test_precision_class_2)],\n",
    "                           \"Test IoU class 1\": [np.mean(test_iou_class_1)],\n",
    "                           \"Test IoU class 2\": [np.mean(test_iou_class_2)],\n",
    "                           })\n",
    "print(results_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}